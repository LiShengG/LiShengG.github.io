---
title: Crash算法
date: 2024-09-10 11:02:45
permalink: /pages/1bfa63/
author: 
  name: lisheng
  link: https://github.com/LiShengG
---
CRUSH算法旨在解决大规模集群中如何有效地存储和定位数据的问题。
通过CRUSH算法，Ceph能够实现去中心化的、无元数据服务器的分布式数据存储。

一个数据算法需要至少满足以下功能：
- 实现数据的随机分布，并在读取时能快速索引。
- 能够高效地重新分布数据，在设备加入、删除和失效时最小化数据迁移。
- 能够根据设备的物理位置合理地控制数据的失效域。
- 支持常见的镜像、磁盘阵列、纠删码等数据安全机制。
- 支持不同存储设备的权重分配，来表示其容量大小或性能。

CRUSH元数据包含了`CRUSH Map`、`OSDMap`和`CRUSH Rule`。
CRUSH Map保存了集群中所有设备或0SD存储节点的位置信息和权重设置，使CRUSH算法能够感知0SD的实际分布和特性，并通过用户定义的CRUSH Rule来保证算法选择出来的位置能够合理分布在不同的失效域中。
OSDMap保存了各个OSD的运行时状态，能够让 CRUSH算法感知存储节点的失效、删除和加入情况，产生最小化的数据迁移，提高Ceph 在各种情况下的可用性和稳定性。

<!-- ![crash](../../img/crash.png) -->

在分布式存储系统Ceph中，数据的放置策略通过一种称为**CRUSH**（Controlled Replication Under Scalable Hashing）算法来实现。CRUSH算法是Ceph的一大核心组件，旨在解决大规模集群中如何有效地存储和定位数据的问题。通过CRUSH算法，Ceph能够实现去中心化的、无元数据服务器的分布式数据存储。



1. **存储节点组织结构：**
   Ceph 将存储节点组织成树状的拓扑结构，称为CRUSH Map。这种结构允许定义不同层级的存储单元，常见的层级有：
   - **Datacenter（数据中心）**
   - **Rack（机架）**
   - **Host（主机）**
   - **OSD（Object Storage Daemon，实际存储单元）**


2. **OSDMap与设备的状态：**
  在运行时期，Ceph的 Monitor会在OSDMap中维护一种所有0SD设备的运行状态，并在集群内同步。其中，0SD运行状态的更新是通过OSD-0SD和OSD-Monitor 的心跳完成的。任何集群状态的变化都会导致
  Monitor 中维护的OSDMap版本号（Epoch）递增，这样Ceph 客户端和OSD服务就可以通过比较版本号大小来判断自己的 Map 是否已经过时，并及时进行更新。
  OSD设备的具体状态可以是在集群中（in)或不在集群中（out)，以及正常运行（up）或处于非正常运行状态（down)。其中 OSD设备的 in、out、up和down状态可以任意组合，只是当0SD同时处于in 和down状态时，表示集群处于不正常状态。在OSD快满时，也会被标记为 full。我们可以通过以下命令查询OSDMap的状态，或者手动标记0SD设备的状态: